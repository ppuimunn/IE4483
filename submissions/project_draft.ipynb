{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hanse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import nltk\n",
    "import ssl\n",
    "import random\n",
    "import re\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I bought this belt for my daughter in-law for ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The size was perfect and so was the color.  It...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fits and feels good, esp. for doing a swim rac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>These socks are absolutely the best. I take pi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you so much for the speedy delivery they...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  sentiments\n",
       "0  I bought this belt for my daughter in-law for ...           1\n",
       "1  The size was perfect and so was the color.  It...           1\n",
       "2  Fits and feels good, esp. for doing a swim rac...           1\n",
       "3  These socks are absolutely the best. I take pi...           1\n",
       "4  Thank you so much for the speedy delivery they...           1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "train_df = pd.read_json(\"../instructions/train.json\")\n",
    "test_df = pd.read_json(\"../instructions/test.json\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # get stopwords\n",
    "    excluded = set(stopwords.words('english'))\n",
    "    \n",
    "    # remove email addresses\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "\n",
    "    # replace '@' with 'at' and '#' with ''\n",
    "    text = text.replace('@', 'at')\n",
    "    text = text.replace('#', '')\n",
    "\n",
    "    # normalize text: lowercase, remove non-alphabetic characters, and extra spaces\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text.lower())\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # filter out stopwords\n",
    "    words = [word for word in text.split() if word not in excluded]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create flags for train and test dfs\n",
    "train_df['flag'] = 'train'\n",
    "test_df['flag'] = 'test'\n",
    "\n",
    "# combine dfs and override reviews with clean text\n",
    "combined_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "combined_df['reviews'] = combined_df['reviews'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13528"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get cleaned train reviews\n",
    "train_reviews = combined_df[combined_df['flag'] == 'train']['reviews']\n",
    "\n",
    "# find no. of unique words in train dataset\n",
    "unique_words = set(word for review in train_reviews for word in review)\n",
    "\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hanse\\miniconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 5920\n",
      "Validation set size: 1481\n",
      "Test set size: 1851\n"
     ]
    }
   ],
   "source": [
    "train_df = combined_df[combined_df['flag'] == 'train'].reset_index(drop=True)\n",
    "test_df = combined_df[combined_df['flag'] == 'test'].reset_index(drop=True)\n",
    "\n",
    "# encode train and test data using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000, tokenizer=lambda x: x, preprocessor=lambda x: x)\n",
    "vectorizer = vectorizer.fit(train_df['reviews'])\n",
    "X_train = vectorizer.transform(train_df['reviews'])\n",
    "X_test = vectorizer.transform(test_df['reviews'])\n",
    "\n",
    "# split the train data into train and validation sets\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train, train_df['sentiments'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {X_train_split.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val_split.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to PyTorch tensors\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "# convert sparse TF-IDF matrices to dense tensors\n",
    "X_train_tensor = torch.tensor(X_train_split.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_split.values, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val_split.toarray(), dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_split.values, dtype=torch.long)\n",
    "\n",
    "# create TensorDatasets and DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "# define model with weights\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, dropout_rate=0.2):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "        # custom weight initialization\n",
    "        init.xavier_uniform_(self.fc1.weight)\n",
    "        init.zeros_(self.fc1.bias)\n",
    "        init.xavier_uniform_(self.fc2.weight)\n",
    "        init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 128          \n",
    "dropout_rate = 0.2       \n",
    "learning_rate = 0.001    \n",
    "batch_size = 64          \n",
    "num_epochs = 10          \n",
    "\n",
    "# create model, criterion, optimizer\n",
    "model = SentimentClassifier(input_dim, hidden_dim, dropout_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.4289, Validation Accuracy: 0.8589, Validation F1-score: 0.7937\n",
      "Epoch 2/10, Loss: 0.2691, Validation Accuracy: 0.8960, Validation F1-score: 0.8704\n",
      "Epoch 3/10, Loss: 0.1587, Validation Accuracy: 0.9257, Validation F1-score: 0.9193\n",
      "Epoch 4/10, Loss: 0.0961, Validation Accuracy: 0.9264, Validation F1-score: 0.9212\n",
      "Epoch 5/10, Loss: 0.0624, Validation Accuracy: 0.9264, Validation F1-score: 0.9210\n",
      "Epoch 6/10, Loss: 0.0422, Validation Accuracy: 0.9190, Validation F1-score: 0.9114\n",
      "Epoch 7/10, Loss: 0.0292, Validation Accuracy: 0.9210, Validation F1-score: 0.9159\n",
      "Epoch 8/10, Loss: 0.0200, Validation Accuracy: 0.9190, Validation F1-score: 0.9131\n",
      "Epoch 9/10, Loss: 0.0141, Validation Accuracy: 0.9183, Validation F1-score: 0.9127\n",
      "Epoch 10/10, Loss: 0.0105, Validation Accuracy: 0.9156, Validation F1-score: 0.9101\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "    val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Validation Accuracy: {val_acc:.4f}, \"\n",
    "          f\"Validation F1-score: {val_f1:.4f}\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
